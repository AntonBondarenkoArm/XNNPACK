// Copyright 2025 Google LLC
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.

$assert(SIMD_SIZE > 0)
#include <assert.h>
#include <stddef.h>
#include <stdint.h>

#include "xnnpack/simd/f32-${ARCH}.h"

#include "xnnpack/common.h"
#include "xnnpack/microparams.h"

void xnn_f32_${"p" if PIXELWISE else ""}avgpool_minmax_ukernel_9p__${ARCH}_u${SIMD_SIZE}(
    size_t output_pixels,
    size_t kernel_elements,
    size_t channels,
    const float** input,
    size_t input_offset,
    const float* zero,
    $if PIXELWISE:
      const float* multiplier,
    float* output,
    size_t input_increment,
    size_t output_increment,
    const struct xnn_f32_scaleminmax_params params[restrict XNN_MIN_ELEMENTS(1)])
{
  assert(output_pixels != 0);
  assert(channels != 0);

  const xnn_simd_f32_t vmin = xnn_set1_f32(params->scalar.min);
  const xnn_simd_f32_t vmax = xnn_set1_f32(params->scalar.max);
  XNN_FORCE_REALIZATION(vmin);
  XNN_FORCE_REALIZATION(vmax);
  $if not PIXELWISE:
    const xnn_simd_f32_t vscale = xnn_set1_f32(params->scalar.scale);
    XNN_FORCE_REALIZATION(vscale);

  do {
    // Start with the previous output as the zero buffer.
    const float* prev_output = zero;

    const float** i = input;

    // Passes 0 - n-1: load the output, add 9 inputs.
    size_t k = kernel_elements;
    for (; k > 9; k -= 9) {
      $for K in range(9):
        const float* i${K} = *i++;
        assert(i${K} != NULL);
        if XNN_UNPREDICTABLE(i${K} != zero) {
          i${K} = (const float*) ((uintptr_t) i${K} + input_offset);
        }

      float* o = output;
      size_t c = channels;
      for (; c >= ${SIMD_SIZE}; c -= ${SIMD_SIZE}) {
        $for K in range(9):
          const xnn_simd_f32_t vi${K} = xnn_loadu_f32(i${K}); i${K} += ${SIMD_SIZE};
        const xnn_simd_f32_t vprev = xnn_loadu_f32(prev_output); prev_output += ${SIMD_SIZE};

        const xnn_simd_f32_t vsum018 = xnn_add_f32(xnn_add_f32(vi0, vi1), vi8);
        const xnn_simd_f32_t vsum23 = xnn_add_f32(vi2, vi3);
        const xnn_simd_f32_t vsum45 = xnn_add_f32(vi4, vi5);
        const xnn_simd_f32_t vsum67 = xnn_add_f32(vi6, vi7);

        const xnn_simd_f32_t vsum2345 = xnn_add_f32(vsum23, vsum45);
        const xnn_simd_f32_t vsum01678 = xnn_add_f32(vsum018, vsum67);
        const xnn_simd_f32_t vsum012345678 = xnn_add_f32(vsum2345, vsum01678);
        const xnn_simd_f32_t vacc = xnn_add_f32(vprev, vsum012345678);
        xnn_storeu_f32(o, vacc); o += ${SIMD_SIZE};
      }
      $if SIMD_SIZE > 1:
        if (c > 0) {
          $for K in range(9):
            const xnn_simd_f32_t vi${K} = xnn_load_tail_f32(i${K}, c);
          const xnn_simd_f32_t vprev = xnn_load_tail_safe_f32(prev_output, c);

          const xnn_simd_f32_t vsum018 = xnn_add_f32(xnn_add_f32(vi0, vi1), vi8);
          const xnn_simd_f32_t vsum23 = xnn_add_f32(vi2, vi3);
          const xnn_simd_f32_t vsum45 = xnn_add_f32(vi4, vi5);
          const xnn_simd_f32_t vsum67 = xnn_add_f32(vi6, vi7);

          const xnn_simd_f32_t vsum2345 = xnn_add_f32(vsum23, vsum45);
          const xnn_simd_f32_t vsum01678 = xnn_add_f32(vsum018, vsum67);
          const xnn_simd_f32_t vsum012345678 = xnn_add_f32(vsum2345, vsum01678);
          const xnn_simd_f32_t vacc = xnn_add_f32(vprev, vsum012345678);
          xnn_store_tail_f32(o, vacc, c);
        }

      // Subsequent passes read from the previous output.
      prev_output = output;
    }

    // Final pass: load the output, add remaining kernel elements, apply scaling/min/max
    $for K in range(9):
      const float* i${K} = ${K} < k ? *i++ : zero;
      assert(i${K} != NULL);
      if XNN_UNPREDICTABLE(i${K} != zero) {
        i${K} = (const float*) ((uintptr_t) i${K} + input_offset);
      }

    $if PIXELWISE:
      xnn_simd_f32_t vscale = xnn_set1_f32(*multiplier++);
    size_t c = channels;
    for (; c >= ${SIMD_SIZE}; c -= ${SIMD_SIZE}) {
      $for K in range(9):
        const xnn_simd_f32_t vi${K} = xnn_loadu_f32(i${K}); i${K} += ${SIMD_SIZE};
      const xnn_simd_f32_t vprev = xnn_loadu_f32(prev_output); prev_output += ${SIMD_SIZE};

      const xnn_simd_f32_t vsum018 = xnn_add_f32(xnn_add_f32(vi0, vi1), vi8);
      const xnn_simd_f32_t vsum23 = xnn_add_f32(vi2, vi3);
      const xnn_simd_f32_t vsum45 = xnn_add_f32(vi4, vi5);
      const xnn_simd_f32_t vsum67 = xnn_add_f32(vi6, vi7);

      const xnn_simd_f32_t vsum2345 = xnn_add_f32(vsum23, vsum45);
      const xnn_simd_f32_t vsum01678 = xnn_add_f32(vsum018, vsum67);
      const xnn_simd_f32_t vsum012345678 = xnn_add_f32(vsum2345, vsum01678);

      xnn_simd_f32_t vacc = xnn_add_f32(vprev, vsum012345678);

      vacc = xnn_mul_f32(vacc, vscale);
      vacc = xnn_max_f32(vacc, vmin);
      vacc = xnn_min_f32(vacc, vmax);

      xnn_storeu_f32(output, vacc); output += ${SIMD_SIZE};
    }
    $if SIMD_SIZE > 1:
      if (c > 0) {
        $for K in range(9):
          const xnn_simd_f32_t vi${K} = xnn_load_tail_f32(i${K}, c);
        const xnn_simd_f32_t vprev = xnn_load_tail_safe_f32(prev_output, c);

        const xnn_simd_f32_t vsum018 = xnn_add_f32(xnn_add_f32(vi0, vi1), vi8);
        const xnn_simd_f32_t vsum23 = xnn_add_f32(vi2, vi3);
        const xnn_simd_f32_t vsum45 = xnn_add_f32(vi4, vi5);
        const xnn_simd_f32_t vsum67 = xnn_add_f32(vi6, vi7);

        const xnn_simd_f32_t vsum2345 = xnn_add_f32(vsum23, vsum45);
        const xnn_simd_f32_t vsum01678 = xnn_add_f32(vsum018, vsum67);
        const xnn_simd_f32_t vsum012345678 = xnn_add_f32(vsum2345, vsum01678);

        xnn_simd_f32_t vacc = xnn_add_f32(vprev, vsum012345678);

        vacc = xnn_mul_f32(vacc, vscale);
        vacc = xnn_max_f32(vacc, vmin);
        vacc = xnn_min_f32(vacc, vmax);

        xnn_store_tail_f32(output, vacc, c); output += c;
      }

    input = (const float**) ((uintptr_t) input + input_increment);
    output = (float*) ((uintptr_t) output + output_increment);
  } while (--output_pixels != 0);
}
